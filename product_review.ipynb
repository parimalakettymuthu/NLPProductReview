{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "787921f8-dbe9-4d5a-9025-b91194f3e0f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reviews = spark.sql(\"SELECT * FROM reviews_2\")\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a82263af-633f-4966-a831-98c9835e7022",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reviews.select(\"Text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de0a6d00-3f35-44b0-80bb-ffe4fa52c371",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reviews.select(\"Text\").first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c91429-8577-4b38-8953-c6b222cc6f51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "score_counts = reviews.groupBy(\"Score\").count().orderBy(\"Score\").toPandas()\n",
    "\n",
    "score_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07bf4001-89b3-4518-ae9a-65cf5777a413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(score_counts[\"Score\"], score_counts[\"count\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Review Count\")\n",
    "plt.title(\"Review Count Based on Score\")\n",
    "#plt.xticks(score_counts[\"Score\"])\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d0fed81-13f5-443d-b836-4807ee5b84a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "reviews = reviews.withColumn(\"ReviewSentiment\", when(reviews[\"Score\"] <=3, \"Negative Review\").otherwise(\"Positive Review\"))\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a6448c8-f5c2-415f-935a-24e5740240e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "review_count = reviews.groupBy(\"ReviewSentiment\").count().orderBy(\"ReviewSentiment\").toPandas()\n",
    "review_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82fcf64d-5ce6-4df3-b5c4-b481d731c2a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(review_count[\"ReviewSentiment\"], review_count[\"count\"], color=\"skyblue\")\n",
    "plt.xlabel(\"ReviewSentiment\")\n",
    "plt.ylabel(\"Review Count\")\n",
    "plt.title(\"Review Count Based on Score\")\n",
    "#plt.xticks(score_counts[\"Score\"])\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c8ba2a-8526-4fbd-aa98-e03350e4f936",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "567a7c11-1f9e-429b-b4d4-3ab6d6fd4a84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "def preprocess_text(text):\n",
    "    #Convert to lowercase \n",
    "    text = text.lower()\n",
    "    #Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    #Tokenization \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    #Stopword removal\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    #Stemming\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "    tokens = [lemmatize.lemmatize(word) for word in tokens]\n",
    "    #join tokens back into text\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40119a4c-b670-4eeb-9f85-ba87e109fe7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, collect_list\n",
    "from pyspark.sql.types import StringType\n",
    "#Register preprocessing function as udf \n",
    "preprocess_udf = udf(preprocess_text, StringType())\n",
    "\n",
    "#Apply preprocessing to \"Text column\"\n",
    "reviews = reviews.withColumn(\"preprocessed_text\", preprocess_udf(reviews[\"Text\"]))\n",
    "\n",
    "#Collect preprocessed reviews as a list \n",
    "preprocessed_reviews = [row.preprocessed_text for row in reviews.select(\"preprocessed_text\").collect()]\n",
    "                        #reviews.agg(collect_list(\"preprocessed_text\")).first()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9ba8ed-732f-47c7-bc47-aa8e77c768c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reviews = reviews.withColumn(\"ReviewSentiment\", when(reviews[\"ReviewSentiment\"]==\"Positive Review\",1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70898dea-05e8-4771-98da-7d008cc5abad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "#Apply pre\n",
    "\n",
    "#Apply preprocessing to each review & Process\n",
    "preprocessed_reviews = [preprocess_text(reviews) for review in reviews[\"Text\"]]\n",
    "\n",
=======
>>>>>>> Stashed changes
    "#Vectorization using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(preprocessed_reviews)\n",
    "y = reviews.select(\"ReviewSentiment\").toPandas()[\"ReviewSentiment\"]\n",
    "\n",
    "#Convert PySpark DataFrame columns to numpy arrays\n",
    "X_array = X.toarray()\n",
    "y_array = y.values\n",
    "#Splitting data into training & test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_array, y_array, test_size=0.2, random_state=42)\n",
    "\n",
    "#Display preprocessed reviews\n",
    "print(\"Preprocessed reviews: \")\n",
    "for review in preprocessed_reviews:\n",
    "    print(review)\n",
    "\n",
    "#Display TF-IDF vectors\n",
    "print(\"\\n TF-IDF vectors:\")\n",
    "print(X.toarray())\n",
    "\n",
    "#Display training and test data shapes\n",
    "print(\"\\nTraining Data Shape: \", X_train.shape)\n",
    "print(\"\\nTest Data Shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d95d38-9f8f-4e14-b85f-7b8c8cce9764",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#Handle imbalance dataset\n",
    "class_weights = {0: 0.2, 1: 0.8}\n",
    "\n",
    "#Choose model\n",
    "review_model = RandomForestClassifier(class_weight=class_weights)\n",
    "\n",
    "#Train the model \n",
    "review_model.fit(X_train, Y_train)\n",
    "\n",
    "#Evaluate the model\n",
    "y_pred = review_model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "classification_report = classification_report(Y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Classification Report: \",classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df545961-d64e-4483-b899-d8d3aef78378",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(class_weight=class_weights),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight=class_weights),\n",
    "    \"Support Vector Machine\": SVC(class_weight=class_weights),\n",
    "    \"Gradient Boosting\":GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    clf.fit(X_train, Y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, y_pred)\n",
    "    report = classification_report(Y_test, y_pred, zero_division=1)\n",
    "\n",
    "    print(f\"\\n {name} Accuracy: {accuracy}\")\n",
    "    print(f\"\\n Classification Report: \")\n",
    "    print(f\"\\n{report}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 311249187624342,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "product_review",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
